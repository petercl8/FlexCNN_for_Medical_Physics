{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdfEsglDIfJ3"
   },
   "source": [
    "## Install Package Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mode='inline'    # Options: 'always' (always show plots), 'inline' (only in Jupyter/Interactive Window), 'never' (silent)\n",
    "device_opt='cuda' # Options: 'sense', cuda', 'cpu'. Set to 'sense' to set to 'cpu' if available, else 'cpu'.\n",
    "\n",
    "# Colab / Local Setup Options ##\n",
    "skip_local_package_installs=True # Local only: skip package checks and pip installs, just reload code. Set to True after first run for faster startup.\n",
    "ray_tune_version=None # Optional: Pin Ray Tune to specific version (e.g., '2.9.0'). Set to None to use latest.\n",
    "skip_colab_git_update=False # Colab only: Skip git pull when setting up repository. Useful if you have uncommitted changes or git operations fail.\n",
    "setup_mode='walk' # Local only: 'walk' for module walking or 'install' for editable install from local repo.\n",
    "base_repo_path=None # Optional: override repo root detection (useful for notebooks).\n",
    "\n",
    "## Github Repository for Functions & Classes ##\n",
    "github_username='petercl8'\n",
    "repo_name='FlexCNN_for_Medical_Physics'\n",
    "\n",
    "## Directories ##\n",
    "project_colab_dirPath = '/content/drive/MyDrive/Colab/Working/'     # Directory, relative to which all other directories are specified (if working on Colab)\n",
    "project_local_dirPath = r'C:\\Users\\Peter Lindstrom\\My Drive (lindstrom.peter@gmail.com)\\Colab\\Working'  # Directory, relative to which all other directories are specified (if working Locally)\n",
    "\n",
    "local_repo_dirPath =  r'C:\\FlexCNN_cloned'\n",
    "\n",
    "data_dirName = 'dataset-sets'      # Dataset directory, placed in project directory (above)\n",
    "plot_dirName=  'plots'             # Plots Directory, placed in project directory (above)\n",
    "checkpoint_dirName='checkpoints'   # If not using Ray Tune (not tuning), PyTorch saves and loads checkpoint file from here\n",
    "                                   # All checkpoint files (for training, testing, visualizing) save the states for a particular network.\n",
    "                                   # Therefore, the hyperparameters for the loaded CNN must match the data in the checkpoint file.\n",
    "num_examples=-1                    # Number of examples from dataset to load. Set to -1 to use all examples (this is the default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TOlPJH8MGx21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ï¸  Skipping package checks (skip_local_package_installs=True)\n",
      "ðŸ“¦ Loading FlexCNN_for_Medical_Physics package (walk mode)...\n",
      "âœ¨ Injecting all symbols into global namespace...\n",
      "âœ… Setup complete: 244 symbols loaded into globals.\n",
      "CPUs available: 20\n",
      "GPUs available: 1\n",
      "  GPU 0: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "[INFO] Plot mode: inline - using interactive backend: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import os, sys, glob, importlib, inspect, types, subprocess, pkgutil\n",
    "\n",
    "def sense_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        IN_COLAB = True\n",
    "    except ImportError:\n",
    "        IN_COLAB = False\n",
    "    return IN_COLAB\n",
    "\n",
    "def sense_device(device='sense'):\n",
    "    if device == 'sense':\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "    elif device == 'cpu':\n",
    "        device = 'cpu'\n",
    "    elif device == 'cuda':\n",
    "        device = 'cuda'\n",
    "    return device\n",
    "\n",
    "def install_packages(IN_COLAB=True, force_reinstall=False, include_optional=True, ray_version=None):\n",
    "    \"\"\"\n",
    "    Installs required Python packages efficiently.\n",
    "    - Detects if running in Colab or locally.\n",
    "    - Installs missing packages only (unless force_reinstall=True).\n",
    "    - For local: always installs CUDA-enabled PyTorch (cu124).\n",
    "    - Optionally pin Ray version with ray_version (e.g., \"2.9.0\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Base list of non-PyTorch packages\n",
    "    other_packages = [\n",
    "        \"ray[tune]\", \"tensorboardX\", \"hyperopt\", \"optuna\",\n",
    "        \"numpy\", \"pandas\", \"matplotlib\",\n",
    "        \"scikit-image\", \"scipy\"\n",
    "    ]\n",
    "\n",
    "    # Optional packages\n",
    "    optional_packages = [\"tensorboard\"]\n",
    "    widgets_packages = [\"ipywidgets\"]\n",
    "\n",
    "    missing = []\n",
    "\n",
    "    # On Colab, just use standard installation\n",
    "    if IN_COLAB:\n",
    "        packages = [\n",
    "            \"torch\", \"torchvision\", \"torchaudio\",\n",
    "            \"ray[tune]\", \"tensorboardX\", \"hyperopt\", \"optuna\",\n",
    "            \"numpy\", \"pandas\", \"matplotlib\",\n",
    "            \"scikit-image\", \"scipy\"\n",
    "        ]\n",
    "        optional_packages_to_install = [\"tensorboard\"] if include_optional else []\n",
    "        widgets_packages_to_install = [\"ipywidgets\"] if include_optional else []\n",
    "        \n",
    "        for pkg in packages + optional_packages_to_install + widgets_packages_to_install:\n",
    "            pkg_name = pkg.split(\"[\")[0]\n",
    "            if pkg_name == \"ray\":\n",
    "                try:\n",
    "                    import ray\n",
    "                    import ray.tune\n",
    "                    ray_tune_installed = True\n",
    "                except ImportError:\n",
    "                    ray_tune_installed = False\n",
    "                \n",
    "                # Pin Ray version if specified\n",
    "                if ray_version:\n",
    "                    pkg = f\"ray[tune]=={ray_version}\"\n",
    "                \n",
    "                if force_reinstall or not ray_tune_installed:\n",
    "                    missing.append(pkg)\n",
    "            elif importlib.util.find_spec(pkg_name) is None or force_reinstall:\n",
    "                missing.append(pkg)\n",
    "        \n",
    "        if not missing:\n",
    "            print(\"âœ… All required packages already installed.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ðŸ“¦ Installing missing packages: {', '.join(missing)}\")\n",
    "        \n",
    "        # For Colab, install PyTorch with CUDA support (cu124 works on Colab)\n",
    "        torch_packages = [p for p in missing if p.split(\"[\")[0] in [\"torch\", \"torchvision\", \"torchaudio\"]]\n",
    "        other_missing = [p for p in missing if p.split(\"[\")[0] not in [\"torch\", \"torchvision\", \"torchaudio\"]]\n",
    "        \n",
    "        if torch_packages:\n",
    "            print(f\"ðŸ“¦ Installing PyTorch with CUDA (cu124) for Colab GPU support...\")\n",
    "            cmd_torch = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--index-url\", \"https://download.pytorch.org/whl/cu124\"] + torch_packages\n",
    "            try:\n",
    "                subprocess.check_call(cmd_torch)\n",
    "                print(\"âœ… PyTorch installation complete.\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"âŒ PyTorch installation failed: {e}\")\n",
    "                return\n",
    "        \n",
    "        if other_missing:\n",
    "            print(f\"ðŸ“¦ Installing other packages...\")\n",
    "            try:\n",
    "                cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"] + other_missing\n",
    "                subprocess.check_call(cmd)\n",
    "                print(\"âœ… Installation complete.\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"âŒ Installation failed: {e}\")\n",
    "                print(\"ðŸ” Retrying installs individually (no cache)...\")\n",
    "                failed = []\n",
    "                for pkg in other_missing:\n",
    "                    try:\n",
    "                        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--no-cache-dir\", pkg])\n",
    "                    except subprocess.CalledProcessError:\n",
    "                        failed.append(pkg)\n",
    "                if failed:\n",
    "                    raise RuntimeError(f\"Failed to install packages: {failed}\")\n",
    "\n",
    "        # Validate Ray is available before importing package modules\n",
    "        try:\n",
    "            import ray  # noqa: F401\n",
    "            import ray.tune  # noqa: F401\n",
    "        except ImportError as e:\n",
    "            raise RuntimeError(\"Ray Tune is required but not installed. Please re-run the install cell.\") from e\n",
    "        return\n",
    "\n",
    "    # Local: Always install CUDA PyTorch (cu124), other packages with standard PyPI\n",
    "    print(\"ðŸ–¥ï¸  Local environment detected. Installing CUDA-enabled PyTorch...\")\n",
    "    \n",
    "    torch_packages = [\"torch\", \"torchvision\", \"torchaudio\"]\n",
    "    \n",
    "    # Check which packages are missing\n",
    "    for pkg in other_packages:\n",
    "        pkg_name = pkg.split(\"[\")[0]\n",
    "        if pkg_name == \"ray\":\n",
    "            try:\n",
    "                import ray\n",
    "                import ray.tune\n",
    "                ray_tune_installed = True\n",
    "            except ImportError:\n",
    "                ray_tune_installed = False\n",
    "            \n",
    "            # Pin Ray version if specified\n",
    "            if ray_version:\n",
    "                pkg = f\"ray[tune]=={ray_version}\"\n",
    "            \n",
    "            if force_reinstall or not ray_tune_installed:\n",
    "                missing.append(pkg)\n",
    "        elif importlib.util.find_spec(pkg_name) is None or force_reinstall:\n",
    "            missing.append(pkg)\n",
    "    \n",
    "    if include_optional:\n",
    "        missing += optional_packages + widgets_packages\n",
    "    \n",
    "    missing = list(dict.fromkeys(missing))\n",
    "    \n",
    "    # Install torch with CUDA index\n",
    "    print(f\"ðŸ“¦ Installing PyTorch with CUDA (cu124)...\")\n",
    "    cmd_torch = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--index-url\", \"https://download.pytorch.org/whl/cu124\"] + torch_packages\n",
    "    try:\n",
    "        subprocess.check_call(cmd_torch)\n",
    "        print(\"âœ… PyTorch installation complete.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ PyTorch installation failed: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Install other packages with standard PyPI\n",
    "    if missing:\n",
    "        print(f\"ðŸ“¦ Installing other packages...\")\n",
    "        cmd_other = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"] + missing\n",
    "        try:\n",
    "            subprocess.check_call(cmd_other)\n",
    "            print(\"âœ… Other packages installation complete.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âŒ Other packages installation failed: {e}\")\n",
    "    \n",
    "    # Diagnose CUDA\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CUDA Diagnostic Information:\")\n",
    "    print(\"=\"*60)\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "        print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"âœ… CUDA device count: {torch.cuda.device_count()}\")\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        else:\n",
    "            print(\"âŒ CUDA is NOT available - this is a problem!\")\n",
    "            print(\"   Checking nvidia-smi...\")\n",
    "            try:\n",
    "                result = subprocess.check_output(\"nvidia-smi\", shell=True).decode()\n",
    "                print(\"   nvidia-smi output:\")\n",
    "                for line in result.split('\\n')[:10]:\n",
    "                    print(f\"     {line}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   nvidia-smi not found: {e}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ PyTorch import failed: {e}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Reload_submodules is used in both setup functions\n",
    "def reload_submodules(pkg):\n",
    "    \"\"\"Reload all submodules in a package to pick up code changes.\"\"\"\n",
    "    for importer, modname, ispkg in pkgutil.walk_packages(pkg.__path__, pkg.__name__ + \".\"):\n",
    "        try:\n",
    "            sub_module = importlib.import_module(modname)\n",
    "            importlib.reload(sub_module)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def resolve_repo_root(base_repo_path=None):\n",
    "    \"\"\"Resolve repo root by searching for setup.py/pyproject.toml.\"\"\"\n",
    "    if base_repo_path:\n",
    "        return base_repo_path\n",
    "\n",
    "    try:\n",
    "        start_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        start_dir = os.getcwd()\n",
    "\n",
    "    repo_root = start_dir\n",
    "    while repo_root != os.path.dirname(repo_root):\n",
    "        if os.path.exists(os.path.join(repo_root, \"setup.py\")) or os.path.exists(os.path.join(repo_root, \"pyproject.toml\")):\n",
    "            return repo_root\n",
    "        repo_root = os.path.dirname(repo_root)\n",
    "\n",
    "    raise FileNotFoundError(\"Could not locate repo root (setup.py or pyproject.toml). Set base_repo_path.\")\n",
    "\n",
    "def setup_colab_environment(\n",
    "    github_username: str = \"peterlabcl8\",\n",
    "    repo_name: str = \"FlexCNN_for_Medical_Physics\",\n",
    "    local_repo_path: str = None,\n",
    "    skip_git_update: bool = False,\n",
    "    verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Setup environment for Colab: clone/pull repo and install via pip.\n",
    "    Injects all package symbols into caller's globals.\n",
    "    \n",
    "    Args:\n",
    "        github_username: GitHub username for the repository\n",
    "        repo_name: Repository name\n",
    "        local_repo_path: Local path (unused for Colab, kept for consistency)\n",
    "        skip_git_update: If True, skip git pull (useful if already up-to-date or if git operations fail)\n",
    "        verbose: Print status messages\n",
    "    \"\"\"\n",
    "    # Determine base directory\n",
    "    base_dir = \"/content\"\n",
    "    repo_path = os.path.join(base_dir, repo_name)\n",
    "    repo_url = f\"https://github.com/{github_username}/{repo_name}.git\"\n",
    "\n",
    "    # Clone or update\n",
    "    if not os.path.exists(repo_path):\n",
    "        if verbose:\n",
    "            print(f\"ðŸ“¦ Cloning {repo_name} into {base_dir}...\")\n",
    "        try:\n",
    "            subprocess.run([\"git\", \"clone\", repo_url], cwd=base_dir, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âš ï¸  Git clone failed: {e}\")\n",
    "            print(f\"   Proceeding without updating repository...\")\n",
    "    elif not skip_git_update:\n",
    "        if verbose:\n",
    "            print(f\"ðŸ”„ Pulling latest changes in {repo_path}...\")\n",
    "        try:\n",
    "            subprocess.run([\"git\", \"pull\"], cwd=repo_path, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âš ï¸  Git pull failed: {e}\")\n",
    "            print(f\"   Proceeding with existing repository...\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"â­ï¸  Skipping git update (skip_git_update=True)\")\n",
    "\n",
    "    # Install package in editable mode\n",
    "    if verbose:\n",
    "        print(\"âš™ï¸ Installing the package in editable mode...\")\n",
    "    try:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\"],\n",
    "                       cwd=repo_path, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âš ï¸  Package installation failed: {e}\")\n",
    "        print(f\"   Attempting to proceed anyway...\")\n",
    "\n",
    "    # Ensure repo path is importable\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.insert(0, repo_path)\n",
    "\n",
    "    # Import the package\n",
    "    package = importlib.import_module(repo_name)\n",
    "\n",
    "    # Reload all submodules\n",
    "    reload_submodules(package)\n",
    "\n",
    "    # Gather all symbols\n",
    "    imported = {}\n",
    "    for _, modname, ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \".\"):\n",
    "        mod = importlib.import_module(modname)\n",
    "        for name, obj in inspect.getmembers(mod):\n",
    "            if not name.startswith(\"_\"):\n",
    "                imported[name] = obj\n",
    "\n",
    "    # Inject symbols into caller's globals\n",
    "    if verbose:\n",
    "        print(\"âœ¨ Injecting all symbols into global namespace...\")\n",
    "    caller_globals = inspect.stack()[1].frame.f_globals\n",
    "    caller_globals.update(imported)\n",
    "    if verbose:\n",
    "        print(f\"âœ… Setup complete: {len(imported)} symbols loaded into globals.\")\n",
    "\n",
    "def setup_local_environment(\n",
    "    repo_name: str = \"FlexCNN_for_Medical_Physics\",\n",
    "    mode: str = \"walk\",\n",
    "    base_repo_path: str = None,\n",
    "    verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Setup environment for local machine: install or walk package.\n",
    "    Injects all package symbols into caller's globals.\n",
    "    \"\"\"\n",
    "    if mode not in (\"walk\", \"install\"):\n",
    "        raise ValueError(f\"setup_mode must be 'walk' or 'install', got '{mode}'.\")\n",
    "\n",
    "    package_root = resolve_repo_root(base_repo_path)\n",
    "\n",
    "    if mode == \"install\":\n",
    "        if verbose:\n",
    "            print(f\"âš™ï¸ Installing the package in editable mode from {package_root}...\")\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\"],\n",
    "                           cwd=package_root, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âš ï¸  Package installation failed: {e}\")\n",
    "            print(\"   Attempting to proceed anyway...\")\n",
    "    \n",
    "    # Add to sys.path\n",
    "    if package_root not in sys.path:\n",
    "        sys.path.insert(0, package_root)\n",
    "        if verbose:\n",
    "            print(f\"ðŸ“‚ Added {package_root} to sys.path\")\n",
    "\n",
    "    # Import and walk the package\n",
    "    if verbose:\n",
    "        print(f\"ðŸ“¦ Loading {repo_name} package ({mode} mode)...\")\n",
    "    package = importlib.import_module(repo_name)\n",
    "    \n",
    "    # Reload all submodules to pick up code changes\n",
    "    reload_submodules(package)\n",
    "    \n",
    "    # Gather all symbols from all modules\n",
    "    imported = {}\n",
    "    for _, modname, ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \".\"):\n",
    "        try:\n",
    "            mod = importlib.import_module(modname)\n",
    "            imported.update({name: obj for name, obj in vars(mod).items() if not name.startswith('_')})\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Inject symbols into caller's globals\n",
    "    if verbose:\n",
    "        print(\"âœ¨ Injecting all symbols into global namespace...\")\n",
    "    caller_globals = inspect.stack()[1].frame.f_globals\n",
    "    caller_globals.update(imported)\n",
    "    if verbose:\n",
    "        print(f\"âœ… Setup complete: {len(imported)} symbols loaded into globals.\")\n",
    "\n",
    "def refresh_repo(\n",
    "    IN_COLAB = True,\n",
    "    repo_name: str = \"FlexCNN_for_Medical_Physics\",\n",
    "    github_username: str = \"petercl8\",\n",
    "    local_repo_path: str = None,\n",
    "    auto_import: bool = True,\n",
    "    verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Clone/pull and install the repo, then optionally auto-import all modules.\n",
    "    Also reloads all submodules to reflect changes without restarting the runtime.\n",
    "    \"\"\"\n",
    "    # --- Determine base directory ---\n",
    "    base_dir = \"/content\" if IN_COLAB else local_repo_path\n",
    "    if base_dir is None:\n",
    "        raise ValueError(\"local_repo_path must be provided if not in Colab\")\n",
    "\n",
    "    repo_path = os.path.join(base_dir, repo_name)\n",
    "    repo_url = (\n",
    "        f\"https://github.com/{github_username}/{repo_name}.git\"\n",
    "        if IN_COLAB\n",
    "        else f\"git@github.com:{github_username}/{repo_name}.git\"\n",
    "    )\n",
    "\n",
    "    # --- Clone or update ---\n",
    "    if not os.path.exists(repo_path):\n",
    "        if verbose:\n",
    "            print(f\"ðŸ“¦ Cloning {repo_name} into {base_dir}...\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], cwd=base_dir, check=True)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"ðŸ”„ Pulling latest changes in {repo_path}...\")\n",
    "        subprocess.run([\"git\", \"pull\"], cwd=repo_path, check=True)\n",
    "\n",
    "    # --- Install package in editable mode ---\n",
    "    if verbose:\n",
    "        print(\"âš™ï¸ Installing the package in editable mode...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\"],\n",
    "                   cwd=repo_path, check=True)\n",
    "\n",
    "    # --- Ensure repo path is importable ---\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.insert(0, repo_path)\n",
    "\n",
    "    # --- Import the package ---\n",
    "    package = importlib.import_module(repo_name)\n",
    "\n",
    "    # --- Reload all submodules recursively ---\n",
    "    def reload_submodules(pkg):\n",
    "        for _, modname, ispkg in pkgutil.walk_packages(pkg.__path__, pkg.__name__ + \".\"):\n",
    "            if modname in sys.modules:\n",
    "                importlib.reload(sys.modules[modname])\n",
    "            else:\n",
    "                importlib.import_module(modname)\n",
    "        importlib.reload(pkg)\n",
    "\n",
    "    reload_submodules(package)\n",
    "\n",
    "    # --- Gather all symbols ---\n",
    "    imported = {}\n",
    "    for _, modname, ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \".\"):\n",
    "        mod = importlib.import_module(modname)\n",
    "        for name, obj in inspect.getmembers(mod):\n",
    "            if not name.startswith(\"_\"):\n",
    "                imported[name] = obj\n",
    "\n",
    "    # --- Inject symbols into caller's globals if requested ---\n",
    "    if auto_import:\n",
    "        if verbose:\n",
    "            print(\"âœ¨ Injecting all symbols into global namespace...\")\n",
    "        caller_globals = inspect.stack()[1].frame.f_globals\n",
    "        caller_globals.update(imported)\n",
    "        if verbose:\n",
    "            print(f\"âœ… Setup complete: {len(imported)} symbols loaded into globals.\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"âœ… Imported {len(imported)} symbols (not injected).\")\n",
    "\n",
    "\"\"\"## Install Packages & Setup Repo\"\"\"\n",
    "\n",
    "# --- Sense environment ---\n",
    "IN_COLAB = sense_colab()\n",
    "\n",
    "# --- Environment-aware install + repo refresh ---\n",
    "if IN_COLAB:\n",
    "    # Colab: install deps, then clone/pull and reload\n",
    "    install_packages(IN_COLAB, ray_version=ray_tune_version)\n",
    "    setup_colab_environment(\n",
    "        github_username=github_username,\n",
    "        repo_name=repo_name,\n",
    "        skip_git_update=skip_colab_git_update\n",
    "    )\n",
    "else:\n",
    "    # Local: optionally install deps, then reload modules\n",
    "    if not skip_local_package_installs:\n",
    "        # Local: install missing packages (full check)\n",
    "        install_packages(IN_COLAB, ray_version=ray_tune_version)\n",
    "    else:\n",
    "        # Local with skip_local_package_installs=True: assume packages already installed\n",
    "        print(\"â­ï¸  Skipping package checks (skip_local_package_installs=True)\")\n",
    "    setup_local_environment(repo_name=repo_name, mode=setup_mode, base_repo_path=base_repo_path)\n",
    "\n",
    "\n",
    "# --- Test Resources ---\n",
    "list_compute_resources()\n",
    "\n",
    "# --- Set main project directory ---\n",
    "project_dirPath = setup_project_dirs(IN_COLAB, project_local_dirPath, project_colab_dirPath, mount_colab_drive=True)\n",
    "\n",
    "# --- Set Device ---\n",
    "device = sense_device(device=device_opt)\n",
    "\n",
    "# --- Configure Plotting ---\n",
    "from FlexCNN_for_Medical_Physics.functions.helper.display_images import configure_plotting\n",
    "configure_plotting(plot_mode=plot_mode)\n",
    "\n",
    "# Build grouped parameter dictionaries #\n",
    "\n",
    "common_settings = {\n",
    "    'run_mode': run_mode,\n",
    "    'device': device,\n",
    "    'use_cache': use_cache,\n",
    "    'cache_max_gb': cache_max_gb,\n",
    "    'cache_dir': cache_dir,\n",
    "    'num_examples': num_examples,\n",
    "    'act_recon1_scale': act_recon1_scale,\n",
    "    'act_recon2_scale': act_recon2_scale,\n",
    "    'act_sino_scale': act_sino_scale,\n",
    "    'act_image_scale': act_image_scale,\n",
    "    'atten_image_scale': atten_image_scale,\n",
    "    'atten_sino_scale': atten_sino_scale,\n",
    "}\n",
    "\n",
    "base_dirs = {\n",
    "    'project_dirPath': project_dirPath,\n",
    "    'plot_dirName': plot_dirName,\n",
    "    'checkpoint_dirName': checkpoint_dirName,\n",
    "    'tune_storage_dirName': tune_storage_dirName,\n",
    "    'tune_dataframe_dirName': tune_dataframe_dirName,\n",
    "    'test_dataframe_dirName': test_dataframe_dirName,\n",
    "    'data_dirName': data_dirName\n",
    "}\n",
    "\n",
    "data_files = {\n",
    "    'tune_act_sino_file': tune_act_sino_file,\n",
    "    'tune_act_image_file': tune_act_image_file,\n",
    "    'tune_act_recon1_file': tune_act_recon1_file,\n",
    "    'tune_act_recon2_file': tune_act_recon2_file,\n",
    "    'tune_atten_image_file': tune_atten_image_file,\n",
    "    'tune_atten_sino_file': tune_atten_sino_file,\n",
    "    'tune_val_act_sino_file': tune_val_act_sino_file,\n",
    "    'tune_val_act_image_file': tune_val_act_image_file,\n",
    "    'tune_val_atten_image_file': tune_val_atten_image_file,\n",
    "    'tune_val_atten_sino_file': tune_val_atten_sino_file,\n",
    "    'tune_qa_act_sino_file': tune_qa_act_sino_file,\n",
    "    'tune_qa_act_image_file': tune_qa_act_image_file,\n",
    "    'tune_qa_hotMask_file': tune_qa_hotMask_file,\n",
    "    'tune_qa_hotBackgroundMask_file': tune_qa_hotBackgroundMask_file,\n",
    "    'tune_qa_coldMask_file': tune_qa_coldMask_file,\n",
    "    'tune_qa_coldBackgroundMask_file': tune_qa_coldBackgroundMask_file,\n",
    "    'tune_qa_backMask_file': None, # Added this line to fix the KeyError\n",
    "    'tune_qa_atten_image_file': tune_qa_atten_image_file,\n",
    "    'tune_qa_atten_sino_file': tune_qa_atten_sino_file,\n",
    "    'train_act_sino_file': train_act_sino_file,\n",
    "    'train_act_image_file': train_act_image_file,\n",
    "    'train_act_recon1_file': train_act_recon1_file,\n",
    "    'train_act_recon2_file': train_act_recon2_file,\n",
    "    'train_atten_image_file': train_atten_image_file,\n",
    "    'train_atten_sino_file': train_atten_sino_file,\n",
    "    'test_act_sino_file': test_act_sino_file,\n",
    "    'test_act_image_file': test_act_image_file,\n",
    "    'test_act_recon1_file': test_act_recon1_file,\n",
    "    'test_act_recon2_file': test_act_recon2_file,\n",
    "    'test_atten_image_file': test_atten_image_file,\n",
    "    'test_atten_sino_file': test_atten_sino_file,\n",
    "    'visualize_act_sino_file': visualize_act_sino_file,\n",
    "    'visualize_act_image_file': visualize_act_image_file,\n",
    "    'visualize_act_recon1_file': visualize_act_recon1_file,\n",
    "    'visualize_act_recon2_file': visualize_act_recon2_file,\n",
    "    'visualize_atten_image_file': visualize_atten_image_file,\n",
    "    'visualize_atten_sino_file': visualize_atten_sino_file,\n",
    "    }\n",
    "\n",
    "mode_files = {\n",
    "    'tune_csv_file': tune_csv_file,\n",
    "    'train_checkpoint_file': train_checkpoint_file,\n",
    "    'test_checkpoint_file': test_checkpoint_file,\n",
    "    'test_csv_file': test_csv_file,\n",
    "    'visualize_checkpoint_file': visualize_checkpoint_file\n",
    "}\n",
    "\n",
    "network_opts = {\n",
    "    'network_type': network_type,\n",
    "    'train_SI': train_SI,\n",
    "    'gen_image_size': gen_image_size,\n",
    "    'gen_sino_size': gen_sino_size,\n",
    "    'gen_image_channels': gen_image_channels,\n",
    "    'gen_sino_channels': gen_sino_channels,\n",
    "    'SI_normalize': SI_normalize,\n",
    "    'IS_normalize': IS_normalize,\n",
    "}\n",
    "\n",
    "tune_opts = {\n",
    "    'tune_exp_name': tune_exp_name,\n",
    "    'tune_scheduler': tune_scheduler,\n",
    "    'tune_dataframe_fraction': tune_dataframe_fraction,\n",
    "    'tune_restore': tune_restore,\n",
    "    'tune_max_t': tune_max_t,\n",
    "    'tune_minutes': tune_minutes,\n",
    "    'tune_metric': tune_metric,\n",
    "    'tune_even_reporting': tune_even_reporting,\n",
    "    'tune_batches_per_report': tune_batches_per_report,\n",
    "    'tune_examples_per_report': tune_examples_per_report,\n",
    "    'tune_augment': tune_augment,\n",
    "    'tune_grace_period': tune_grace_period,\n",
    "    'tune_debug': tune_debug,\n",
    "    'tune_force_fixed_config': tune_force_fixed_config,\n",
    "    'tune_report_for': tune_report_for,\n",
    "    'tune_qa_hot_weight': tune_qa_hot_weight,\n",
    "    'tune_eval_batch_size': tune_eval_batch_size,\n",
    "    'num_CPUs': num_CPUs,\n",
    "    'num_GPUs': num_GPUs,\n",
    "    'cpus_per_trial': CPUs_per_trial,  # per trial\n",
    "    'gpus_per_trial': GPUs_per_trial,  # per trial\n",
    "    'tune_search_alg': tune_search_alg,\n",
    "}\n",
    "\n",
    "train_opts = {\n",
    "    'train_load_state': train_load_state,\n",
    "    'train_save_state': train_save_state,\n",
    "    'training_epochs': train_epochs,\n",
    "    'train_augment': train_augment,\n",
    "    'train_shuffle': train_shuffle,\n",
    "    'train_display_step': train_display_step,\n",
    "    'train_sample_division': train_sample_division,\n",
    "    'train_show_times': train_show_times,\n",
    "}\n",
    "\n",
    "test_opts = {\n",
    "    'test_display_step': test_display_step,\n",
    "    'test_batch_size': test_batch_size,\n",
    "    'test_chunk_size': test_chunk_size,\n",
    "    'testset_size': testset_size,\n",
    "    'test_begin_at': test_begin_at,\n",
    "    'test_compute_MLEM': test_compute_MLEM,\n",
    "    'test_merge_dataframes': test_merge_dataframes,\n",
    "    'test_show_times': test_show_times,\n",
    "    'test_shuffle': test_sample_division,\n",
    "    'test_sample_division': test_sample_division\n",
    "}\n",
    "\n",
    "viz_opts = {\n",
    "    'visualize_batch_size': visualize_batch_size,\n",
    "    'visualize_offset': visualize_offset,\n",
    "    'visualize_shuffle': visualize_shuffle,\n",
    "}\n",
    "\n",
    "# Build paths and settings using new functions\n",
    "paths = setup_paths(\n",
    "    run_mode=run_mode,\n",
    "    base_dirs=base_dirs,\n",
    "    data_files=data_files,\n",
    "    mode_files=mode_files,\n",
    "    test_ops=test_opts,\n",
    "    viz_ops=viz_opts\n",
    ")\n",
    "\n",
    "settings = setup_settings(\n",
    "    run_mode=run_mode,\n",
    "    common_settings=common_settings,\n",
    "    tune_opts=tune_opts,\n",
    "    train_opts=train_opts,\n",
    "    test_opts=test_opts,\n",
    "    viz_opts=viz_opts,\n",
    ")\n",
    "\n",
    "# --- Build Config Dictionary ---\n",
    "config = construct_config(\n",
    "    run_mode=run_mode,\n",
    "    network_opts=network_opts,\n",
    "    tune_opts=tune_opts,\n",
    "    test_opts=test_opts,\n",
    "    viz_opts=viz_opts,\n",
    "    config_ACT_SI=config_ACT_SI,\n",
    "    config_ACT_IS=config_ACT_IS,\n",
    "    config_ATTEN_SI=config_ATTEN_SI,\n",
    "    config_ATTEN_IS=config_ATTEN_IS,\n",
    "    config_CONCAT=config_CONCAT,\n",
    "    config_FROZEN_COFLOW=config_FROZEN_COFLOW,\n",
    "    config_FROZEN_COUNTERFLOW=config_FROZEN_COUNTERFLOW,\n",
    "    config_GAN_SI=None,\n",
    "    config_GAN_IS=None,\n",
    "    config_RAY_SI=config_RAY_SI,\n",
    "    config_RAY_SI_learnScale=config_RAY_SI_learnScale,\n",
    "    config_RAY_SI_fixedScale=config_RAY_SI_fixedScale,\n",
    "    config_RAY_IS=config_RAY_IS,\n",
    "    config_RAY_IS_learnScale=config_RAY_IS_learnScale,\n",
    "    config_RAY_IS_fixedScale=config_RAY_IS_fixedScale,\n",
    "    config_RAY_SUP=config_RAY_SUP,\n",
    "    config_RAY_SUP_FROZEN=config_RAY_SUP_FROZEN,\n",
    "    config_RAY_GAN=None,\n",
    "    config_RAY_GAN_CYCLE=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tunHHb885PKf"
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9nGISvNMw09"
   },
   "source": [
    "## Compute Recon Scales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uKRUXmeeMWYw"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m avg_activity = compute_average_activity_per_image(\u001b[43mpaths\u001b[49m, dataset=\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#scales1 = compute_quantitative_reconstruction_scale(paths, dataset='train', compute_sinogram_scale=False)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#scales2 = analyze_reconstruction_scale_distribution(paths, dataset='train', sample_mode='full', sample_size=1000, ratio_cap_multiple=None)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#sinogram_scale = compute_sinogram_to_image_scale(paths, dataset='train', sample_number=1000)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'paths' is not defined"
     ]
    }
   ],
   "source": [
    "#avg_activity = compute_average_activity_per_image(paths, dataset='train')\n",
    "#scales1 = compute_quantitative_reconstruction_scale(paths, dataset='train', compute_sinogram_scale=False)\n",
    "#scales2 = analyze_reconstruction_scale_distribution(paths, dataset='train', sample_mode='full', sample_size=1000, ratio_cap_multiple=None)\n",
    "#sinogram_scale = compute_sinogram_to_image_scale(paths, dataset='train', sample_number=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rUBhT1lARKm"
   },
   "source": [
    "**Absolute Activities (Training Set)**\n",
    "\n",
    "Annihilation: Average counts per image: 8676873\n",
    "\n",
    "Activity\n",
    "\n",
    "*   Average activity per image: 144616.84\n",
    "*   Std dev activity per image: 89057.51\n",
    "*   Min activity per image: 13.16\n",
    "*   Max activity per image: 640712.82\n",
    "\n",
    "Recon1: Average activity per image: 43167.45\n",
    "\n",
    "Recon2: Average activity per image: 72402.20\n",
    "\n",
    "Attenuation: 469.025\n",
    "\n",
    "**Activity Sinogram Scale**\n",
    "\n",
    "*   sample_sinogram_number=100, clip_percentile_low=10, clip_percentile_high=90, 'median': 0.341\n",
    "\n",
    "\n",
    "**Quantitative Reconstruction Scales:**\n",
    "\n",
    "Training Set:\n",
    "\n",
    "*   FORE: 3.350\n",
    "*   Oblique: 1.997\n",
    "*   Atten: 308.335\n",
    "\n",
    "Test Set:\n",
    "\n",
    "*   FORE: 3.412\n",
    "*   Oblique: 2.014\n",
    "\n",
    "**Analysis Scales for No Cap:**\n",
    "\n",
    "Training set:\n",
    "\n",
    "*   FORE: 3.400 +/- 0.305 (max_ratio = 4.768)\n",
    "*   Oblique: 2.025 +/- 0.077 (max_ratio = 2.658)\n",
    "\n",
    "Test set:\n",
    "\n",
    "*   FORE: 3.426 +/- 0.292 (max_ratio = 4.727)\n",
    "*   Oblique: 2.034 +/- 0.073 (max_ratio = 2.683)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngD0fv5BD-q8"
   },
   "source": [
    "## Adjust Atten Sino Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_BU2u7MD-q8"
   },
   "outputs": [],
   "source": [
    "# --- Refresh Repository ---\n",
    "refresh_repo(IN_COLAB, local_repo_path=local_repo_dirPath)\n",
    "\n",
    "visualize_sinogram_alignment(\n",
    "    paths,\n",
    "    settings,\n",
    "    num_examples=3,\n",
    "    scale_num_examples=3, # Set to a large number to get an accurate scale\n",
    "    start_index=500,\n",
    "    randomize=True,\n",
    "    random_seed=None,\n",
    "    fig_size=3,\n",
    "    cmap='inferno',\n",
    "    circle=False,\n",
    "    theta_type='symmetrical', # Set to 'speed' to match activity sinogram angular sampling after pooling\n",
    "                        # Set to 'symmetrical' to match sampling before pooling.\n",
    "    # Activity resize/pad options\n",
    "    act_resize_type='crop_pad',   # 'crop_pad', 'bilinear', or None\n",
    "    act_pad_type='zeros', # 'sinoram' or 'zeros'\n",
    "    act_vert_size=320,\n",
    "    act_target_width=320,\n",
    "    act_pool_size=1,\n",
    "    # Attenuation resize/pad options\n",
    "    atten_resize_type='crop_pad', # 'crop_pad', 'bilinear', or None\n",
    "    atten_pad_type='zeros',\n",
    "    atten_vert_size=320,\n",
    "    atten_target_width=320,\n",
    "    atten_pool_size=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfAxOTPrm-6x"
   },
   "outputs": [],
   "source": [
    "print(paths['train_sino_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5qkU0fpHsKa"
   },
   "source": [
    "## Plot Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9PCpxBnHo07"
   },
   "outputs": [],
   "source": [
    "# --- Refresh Repository ---\n",
    "#refresh_repo(IN_COLAB, local_repo_path=local_repo_dirPath)\n",
    "\n",
    "## Anthropomorphic phantoms ##\n",
    "##############################\n",
    "fig_size_XCAT=2.5\n",
    "indexes_XCAT = [100, 200, 300, 400, 500, 600, 700]\n",
    "\n",
    "test_array_names = ['test-actMap.npy', 'test-obliqueImage.npy', 'test-highCountImage.npy',]\n",
    "test_sino = 'test-highCountSino-180x180.npy'\n",
    "#test_sino = 'test-highCountImage.npy'\n",
    "\n",
    "train_array_names = ['train-actMap.npy','train-obliqueImage.npy', 'train-highCountImage.npy', ]\n",
    "train_sino = 'train-highCountSino-180x180.npy'\n",
    "#train_sino = 'train-highCountImage.npy'\n",
    "\n",
    "## QA phantoms ##\n",
    "#################\n",
    "fig_size_QA=2.5\n",
    "indexes_QA = [13, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "NEMA_array_names = ['QA-NEMA-actMap.npy','QA-NEMA-obliqueImage.npy', 'QA-NEMA-highCountImage.npy',]\n",
    "NEMA_sino = 'QA-NEMA-highCountSino.npy'\n",
    "#NEMA_sino = 'QA-NEMA-highCountImage.npy'\n",
    "\n",
    "Pinwheel_array_names = ['QA-Pinwheel-actMap.npy','QA-Pinwheel-obliqueImage.npy', 'QA-Pinwheel-highCountImage.npy']\n",
    "Pinwheel_sino = 'QA-Pinwheel-highCountSino.npy'\n",
    "#Pinwheel_sino = 'QA-Pinwheel-highCountImage.npy'\n",
    "\n",
    "Radial_array_names = ['QA-Radial-actMap.npy','QA-Radial-obliqueImage.npy', 'QA-Radial-highCountImage.npy']\n",
    "Radial_sino = 'QA-Radial-highCountSino.npy'\n",
    "#Radial_sino = 'QA-Radial-highCountImage.npy'\n",
    "\n",
    "#actMap = 'QA-Axial-actMap.npy'\n",
    "#sino = 'QA-Axial-highCountSino.npy'\n",
    "\n",
    "## Network Configs ##\n",
    "#####################\n",
    "\n",
    "checkpoint_fileName_SI = 'checkpoint-fullSet-highCountSino2actMap-tunedSSIM-augII-100epochs'\n",
    "config_SI = {\n",
    "   \"SI_dropout\": False,\n",
    "  \"SI_exp_kernel\": 3,\n",
    "  \"SI_fixedScale\": 1,\n",
    "  \"SI_gen_fill\": 1,\n",
    "  \"SI_gen_final_activ\": nn.ELU(alpha=1.0),\n",
    "  \"SI_gen_hidden_dim\": 29,\n",
    "  \"SI_gen_mult\": 1.5090047574838394,\n",
    "  \"SI_gen_neck\": 11,\n",
    "  \"SI_gen_z_dim\": 486,\n",
    "  \"SI_layer_norm\": \"instance\",\n",
    "  \"SI_learnedScale_init\": 20.45467480669682,\n",
    "  \"SI_normalize\": False,\n",
    "  \"SI_pad_mode\": \"zeros\",\n",
    "  \"SI_skip_mode\": \"none\",\n",
    "  \"batch_base2_exponent\": 6,\n",
    "  \"gen_b1\": 0.34632557248900636,\n",
    "  \"gen_b2\": 0.10963336318792913,\n",
    "  \"gen_lr\": 0.0005750756280291565,\n",
    "  \"gen_image_channels\": 1,\n",
    "  \"gen_image_size\": 180,\n",
    "  \"network_type\": \"SUP\",\n",
    "  \"gen_sino_channels\": 3,\n",
    "  \"sino_size\": 180,\n",
    "  \"sup_criterion\": nn.MSELoss(),\n",
    "  \"train_SI\": True\n",
    "}\n",
    "\n",
    "####################\n",
    "### Plots Images ###\n",
    "####################\n",
    "\n",
    "#image_tensor, sino_tensor = PlotPhantomRecons(test_array_names, test_sino, config_SI, paths, indexes_XCAT, checkpoint_fileName_SI, fig_size_XCAT, device, settings)\n",
    "#show_single_unmatched_tensor(sino_tensor[0:2], fig_size=10)\n",
    "\n",
    "#image_tensor, sino_tensor = PlotPhantomRecons(NEMA_array_names, NEMA_sino, config_SI, paths, indexes_QA, checkpoint_fileName_SI, fig_size_QA, device, settings)\n",
    "image_tensor, sino_tensor = PlotPhantomRecons(Pinwheel_array_names, Pinwheel_sino, config_SI, paths, indexes_QA, checkpoint_fileName_SI, fig_size_QA, device, settings)\n",
    "show_single_unmatched_tensor(sino_tensor[0:2], fig_size=25)\n",
    "\n",
    "#image_tensor, sino_tensor = PlotPhantomRecons(Radial_array_names, Radial_sino, config_SI, paths, indexes_QA, checkpoint_fileName_SI, fig_size_QA, device, settings)\n",
    "\n",
    "#############\n",
    "## Metrics ##\n",
    "#############\n",
    "\n",
    "#frame_SSIM_MLEM, placeholder = calculate_metric(MLEM_output, image_tensor, SSIM, dataframe = True, label='MLEM, SSIM')\n",
    "#frame_MSE_MLEM, placeholder =  calculate_metric(MLEM_output, image_tensor, MSE, dataframe = True, label='MLEM, MSE')\n",
    "#print('################### MLEM ###################')\n",
    "#print(frame_SSIM_MLEM.T)\n",
    "#print(frame_MSE_MLEM.T)\n",
    "\n",
    "break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jkVa95flUdo"
   },
   "source": [
    "## Tuning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiM_F1KL1cLd"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## Set Options ##\n",
    "#################\n",
    "\n",
    "tune_exp_name=\"/content/drive/MyDrive/Colab/Working/searches/search-fullSet-highCountSino2actMap-tunedSSIM-AugmentSI\"\n",
    "save_fig=True\n",
    "\n",
    "titlesize=13\n",
    "fontsize=12\n",
    "ticksize=10\n",
    "dpi=800\n",
    "figsize=(10,8)\n",
    "\n",
    "fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "gs = gridspec.GridSpec(ncols=100, nrows=100)\n",
    "\n",
    "# Top Row Axes #\n",
    "ax1 = fig.add_subplot(gs[0:25,   0:100])\n",
    "ax2 = fig.add_subplot(gs[38:62,   0:100])\n",
    "#ax3 = fig.add_subplot(gs[75:100,  0:100])\n",
    "\n",
    "\n",
    "## Plots ###\n",
    "\n",
    "#refresh_repo(IN_COLAB, local_repo_path=local_repo_dirPath)\n",
    "\n",
    "result_grid, bestResult_logDir = PlotFrame(paths, tune_exp_name, ax1, 'batch_step', 'Batch Step', 'MSE', 'MSE', ylim=(50,20000), logy=True)\n",
    "ax1.set_title('(A) MSE Learning Curves', fontsize=titlesize)\n",
    "\n",
    "result_grid, bestResult_logDir = PlotFrame(paths, tune_exp_name, ax2, 'batch_step', 'Batch Step', 'SSIM', 'SSIM', ylim=(0,1), logy=False)\n",
    "ax2.set_title('(B) SSIM Learning Curves', fontsize=titlesize)\n",
    "\n",
    "#result_grid, bestResult_logDir = PlotFrame(paths, tune_exp_name, ax3, 'batch_step', 'Batch Step', 'CUSTOM', 'Local Distributions Metric', ylim=(300,500))\n",
    "#ax3.set_title('(A) LDM Learning Curves', fontsize=titlesize)\n",
    "\n",
    "# Best Result #\n",
    "\n",
    "#print(bestResult_logdir)\n",
    "\n",
    "# Save Fig? #\n",
    "\n",
    "if save_fig:\n",
    "    plot_save_name='figure-tuning'\n",
    "    savefig(os.path.join(paths['plot_dirPath'], plot_save_name+'.svg'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-RDucbz-NOi"
   },
   "source": [
    "## Tune Frame Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sPXBVEODKlR"
   },
   "outputs": [],
   "source": [
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hW_RJkIi-XaT"
   },
   "outputs": [],
   "source": [
    "\n",
    "tune_csv_file = 'frame-tunedMSE-ASHA'\n",
    "\n",
    "\n",
    "tune_dataframe = pd.read_csv(os.path.join(paths['tune_dataframe_dirPath'], tune_csv_file+'.csv'))\n",
    "\n",
    "## Describe Dataframes ##\n",
    "\n",
    "#plt.scatter(tune_dataframe['num_params'], tune_dataframe['mean_CNN_MSE'])\n",
    "#plt.scatter(tune_dataframe['num_params'][1:], tune_dataframe['mean_CNN_MSE'][1:])\n",
    "\n",
    "tune_dataframe.plot.scatter('num_params', 'mean_CNN_MSE', ylim=(0,5))\n",
    "tune_dataframe.plot.scatter('gen_lr', 'mean_CNN_MSE', ylim=(0,5))\n",
    "tune_dataframe.plot.scatter('batch_size', 'mean_CNN_MSE', ylim=(0,5))\n",
    "\n",
    "'''\n",
    "plt.scatter(tune_dataframe['num_params'], tune_dataframe['mean_CNN_MSE'], ylim=(0,1))\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "tune_dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFWg1RoN4NVJ"
   },
   "source": [
    "## Load: Test Dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mMOMILr4NAr"
   },
   "outputs": [],
   "source": [
    "# tunedMSE #\n",
    "test_dataframe_dirPath1= '/content/drive/MyDrive/Colab/Working/Dataframes-TestOnFull'\n",
    "#test_csv_file1 = 'combined-tunedFullMSE-trainedFull-onTrainingSet-noMLEM'   # Use this dataframe to determine thresholds for sorting training set by metrics\n",
    "#test_csv_file1 = 'combined-tunedFullMSE-trainedFull-onTestSet-wMLEM'       # Use this dataframe to determine thresholds for sorting test set by metrics\n",
    "#test_csv_file1 = 'combined-tunedFullSSIM-trainedFull-onTestSet-wMLEM'\n",
    "test_csv_file1 = 'combined-tunedHighMSE-trainedHighMSE-onTestSet-wMLEM'\n",
    "\n",
    "#test_dataframe_dirPath2= '/content/drive/MyDrive/Colab/Working/Dataframes-Test-Quartile-MSE'\n",
    "#test_dataframe_dirPath2= '/content/drive/MyDrive/Colab/Working/Dataframes-TestOnFull'\n",
    "#test_csv_file2 = 'combined-tunedFullSSIM-trainedFull-onTestSet-wMLEM'\n",
    "#test_csv_file2 = 'combined-tunedHighMSE-trainedHighMSE-onTestSet-wMLEM'\n",
    "test_csv_file2 = 'combined-tunedLowSSIM-trainedLowSSIM-onTestSet-wMLEM'\n",
    "\n",
    "# Read Dataframes from File #\n",
    "dataframe_path1 = os.path.join(test_dataframe_dirPath1, test_csv_file1+'.csv')\n",
    "dataframe1 = pd.read_csv(dataframe_path1)\n",
    "dataframe_path2 = os.path.join(test_dataframe_dirPath2, test_csv_file2+'.csv')\n",
    "dataframe2 = pd.read_csv(dataframe_path2)v\n",
    "\n",
    "## Describe Dataframes ##\n",
    "\n",
    "#frame_picked = dataframe[dataframe[\"SSIM (ML-EM)\"]>dataframe[\"SSIM (FBP)\"]]\n",
    "#frame_picked = dataframe[dataframe[\"SSIM (Network)\"]>dataframe[\"SSIM (ML-EM)\"]]\n",
    "\n",
    "#frame_picked = dataframe[dataframe[\"MSE (Network)\"]<dataframe[\"MSE (ML-EM)\"]]\n",
    "#frame_picked = dataframe[dataframe[\"MSE (ML-EM)\"]<dataframe[\"MSE (FBP)\"]]\n",
    "\n",
    "#frame_picked = dataframe1[dataframe1[\"MSE (FBP)\"]>0.95908]\n",
    "#frame_picked = dataframe1[dataframe1[\"MSE (FBP)\"]<0.330922]\n",
    "frame_picked = dataframe1[dataframe1[\"SSIM (FBP)\"]<0.837850]\n",
    "\n",
    "#dataframe1.describe()\n",
    "dataframe2.describe()\n",
    "#frame_picked.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAmVEnJChNGK"
   },
   "source": [
    "### Plot Test Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJbGBeJbKYt7"
   },
   "outputs": [],
   "source": [
    "## Specify Plotting Parameters ##\n",
    "plot_type = 2 # 1 = histograms, 2 = bin plots, 3 = both\n",
    "\n",
    "column_MSE_1 = 'MSE (ML-EM)'\n",
    "#column_MSE_1 = 'MSE (FBP)'\n",
    "column_MSE_2 = 'MSE (Network)'\n",
    "#column_MSE_2 = 'MSE (FBP)'\n",
    "\n",
    "column_SSIM_1 = 'SSIM (ML-EM)'\n",
    "#column_SSIM_1 = 'SSIM (FBP)'\n",
    "column_SSIM_2 = 'SSIM (Network)'\n",
    "#column_SSIM_2 = 'SSIM (FBP)'\n",
    "\n",
    "\n",
    "titlesize=12\n",
    "fontsize=9\n",
    "ticksize=7\n",
    "dpi=800\n",
    "\n",
    "if plot_type == 1 or plot_type == 2:\n",
    "    figsize=(8,6) # 17,5\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    gs = gridspec.GridSpec(ncols=100, nrows=100)\n",
    "\n",
    "    # Top Row Axes #\n",
    "    ax1 = fig.add_subplot(gs[0:42,   0:43])\n",
    "    ax2 = fig.add_subplot(gs[0:42,   57:100])\n",
    "\n",
    "    # Bottom Row Axes #\n",
    "    ax3 = fig.add_subplot(gs[58:100, 0:43])\n",
    "    ax4 = fig.add_subplot(gs[58:100, 57:100])\n",
    "\n",
    "    if plot_type == 1:\n",
    "        plot_hist_1D(ax1, dataframe1, '(1) CNN-A: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2, xlim=(0,4), ylim=(0,5000), bins=40)\n",
    "        plot_hist_1D(ax2, dataframe1, '(2) CNN-A: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n",
    "        plot_hist_1D(ax3, dataframe2, '(3) CNN-B: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2,  xlim=(0,4), ylim=(0,5000),  bins=40)\n",
    "        plot_hist_1D(ax4, dataframe2, '(4) CNN-B: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n",
    "    if plot_type == 2:\n",
    "        plot_hist_2D(ax1, dataframe1, '(1) CNN-A: MSE Bin Plot', column_MSE_1, 'MSE (CNN-A)', column_MSE_1 , column_MSE_2,(0,1.5), (0,1.5), gridsize=60)\n",
    "        plot_hist_2D(ax2, dataframe1, '(2) CNN-A: SSIM Bin Plot',column_SSIM_1, 'SSIM (CNN-A)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n",
    "        plot_hist_2D(ax3, dataframe2, '(3) CNN-B: MSE Bin Plot', column_MSE_1, 'MSE (CNN-B)', column_MSE_1 , column_MSE_2, (0,1.5), (0,1.5), gridsize=60)\n",
    "        plot_hist_2D(ax4, dataframe2, '(4) CNN-B: SSIM Bin Plot', column_SSIM_1, 'SSIM (CNN-B)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n",
    "\n",
    "if plot_type == 3:\n",
    "    figsize=(15,6) # 17,5\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    gs = gridspec.GridSpec(ncols=100, nrows=100)\n",
    "\n",
    "    # Top Row Axes #\n",
    "    ax1 = fig.add_subplot(gs[0:42,   0:18]) # 20\n",
    "    ax2 = fig.add_subplot(gs[0:42,   25:47]) # 22\n",
    "    ax3 = fig.add_subplot(gs[0:42,   53:74]) # 20\n",
    "    ax4 = fig.add_subplot(gs[0:42,   80:100]) # 22\n",
    "\n",
    "    # Bottom Row Axes #\n",
    "    ax5 = fig.add_subplot(gs[58:100, 0:18]) # -5-\n",
    "    ax6 = fig.add_subplot(gs[58:100, 25:47]) # -3 - -3-\n",
    "    ax7 = fig.add_subplot(gs[58:100, 53:74]) # -5-\n",
    "    ax8 = fig.add_subplot(gs[58:100, 80:100])\n",
    "\n",
    "    plot_hist_1D(ax1, dataframe1, '(1) CNN-A: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2, xlim=(0,4), ylim=(0,5000), bins=40)\n",
    "    plot_hist_1D(ax2, dataframe1, '(3) CNN-A: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n",
    "    plot_hist_2D(ax3, dataframe1, '(5) CNN-A: MSE Bin Plot', column_MSE_1, 'MSE (CNN-A)', column_MSE_1 , column_MSE_2,(0,1.5), (0,1.5), gridsize=60)\n",
    "    plot_hist_2D(ax4, dataframe1, '(7) CNN-A: SSIM Bin Plot',column_SSIM_1, 'SSIM (CNN-A)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n",
    "\n",
    "    plot_hist_1D(ax5, dataframe2, '(2) CNN-B: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2,  xlim=(0,4), ylim=(0,5000),  bins=40)\n",
    "    plot_hist_1D(ax6, dataframe2, '(4) CNN-B: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n",
    "    plot_hist_2D(ax7, dataframe2, '(6) CNN-B: MSE Bin Plot', column_MSE_1, 'MSE (CNN-B)', column_MSE_1 , column_MSE_2, (0,1.5), (0,1.5), gridsize=60)\n",
    "    plot_hist_2D(ax8, dataframe2, '(8) CNN-B: SSIM Bin Plot', column_SSIM_1, 'SSIM (CNN-B)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n",
    "\n",
    "save_path = plot_dir+'figure-histograms.png'\n",
    "savefig(save_path, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfTfEgU5cZQl"
   },
   "source": [
    "# Alter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeUpOl4OsFOp"
   },
   "source": [
    "## Sample/Trim/Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBUUjKvNsD3Q"
   },
   "outputs": [],
   "source": [
    "# --- Refresh Repository ---\n",
    "#refresh_repo(IN_COLAB, local_repo_path=local_repo_dirPath)\n",
    "\n",
    "sample_trim_resize(\n",
    "    input_dir='/content/drive/MyDrive/Colab/Working/dataset-sets',\n",
    "    input_file='train-highCountSino-382x513.npy',\n",
    "    output_dir='/content/drive/MyDrive/Colab/Working/dataset-sets',\n",
    "    output_file='train-highCountSino-320x257.npy',\n",
    "    sample_division=1,\n",
    "    remove_n=0,\n",
    "    crop_h=320,\n",
    "    crop_w=None,\n",
    "    new_height=None,\n",
    "    new_width=None,\n",
    "    pool_h_factor=None,\n",
    "    pool_w_factor=2,\n",
    "    seed=0,                # default seed for reproducibility\n",
    "    log_file=\"log-SampleTrimResize.txt\",\n",
    "    skip_if_exists_vm=False,\n",
    "    dry_run=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlNqa1M59MLj"
   },
   "source": [
    "## Precompute Attenuation Sinograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmVi-LjT9Omz"
   },
   "outputs": [],
   "source": [
    "precompute=True\n",
    "\n",
    "#atten_image_fileName = 'train-attenMap.npy'\n",
    "#atten_sino_fileName = 'train-attenSino-382x513.npy'\n",
    "atten_image_fileName = 'test-attenMap.npy'\n",
    "atten_sino_fileName = 'test-attenSino-382x513.npy'\n",
    "\n",
    "\n",
    "if precompute:\n",
    "    precompute_atten_sinos(\n",
    "        project_dirPath,\n",
    "        data_dirName,\n",
    "        atten_image_fileName,\n",
    "        atten_sino_fileName,\n",
    "        sino_height=382,\n",
    "        sino_width=512,\n",
    "        theta_type='symmetrical',\n",
    "        atten_creation_pool_size=2,\n",
    "        circle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-tDENcFyMMC"
   },
   "source": [
    "## Sort: Dataset by Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2q2L7hFyQ1d"
   },
   "outputs": [],
   "source": [
    "## Changeable Variables ##\n",
    "\n",
    "load_sino_path = '/content/drive/MyDrive/Repository/PET_Data/train_sino-70k.npy'\n",
    "load_image_path = '/content/drive/MyDrive/Repository/PET_Data/train_image-70k.npy'\n",
    "save_sino_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_sino-lowSSIM-17500.npy'\n",
    "save_image_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_image-lowSSIM-17500.npy'\n",
    "'''\n",
    "metric_function = MSE\n",
    "max_save_index = 17500\n",
    "threshold = 0.330922\n",
    "threshold_min_max = 'min'\n",
    "'''\n",
    "metric_function = SSIM\n",
    "max_save_index = 17500\n",
    "threshold = 0.837850 #0.837850  # MSE (min): 0.330922, SSIM (max): 0.837850\n",
    "threshold_min_max = 'max'\n",
    "\n",
    "## Run & Verify Result ##\n",
    "save_sino_array, save_image_array = sort_DataSet(config, load_image_path, load_sino_path, save_image_path, save_sino_path, max_save_index,\n",
    "                                                 metric_function, threshold, threshold_min_max=threshold_min_max, num_examples=-1, visualize=False, sino_scale=sino_scale)\n",
    "\n",
    "sino_ground_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwU5ioxR6bHg"
   },
   "source": [
    "## Sort: Check & Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQCMCuDx6bEp"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTrqg3MHNl1x"
   },
   "outputs": [],
   "source": [
    "#save_sino_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_sino-lowSSIM-17500.npy'\n",
    "#save_image_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_sino-lowSSIM-17500.npy'\n",
    "\n",
    "# Print sorted array shape & display a few images #\n",
    "print('save_sino_array.shape: ', save_sino_array.shape)\n",
    "print('save_image_array.shape: ', save_image_array.shape)\n",
    "\n",
    "print('save_sino_array sample images')\n",
    "print('save_image_array sample images')\n",
    "show_multiple_matched_tensors(torch.from_numpy(save_sino_array[500:509]))\n",
    "show_multiple_matched_tensors(torch.from_numpy(save_image_array[500:509]))\n",
    "\n",
    "\n",
    "# Save the sorted array to disk #\n",
    "save_sino_array.flush()\n",
    "save_image_array.flush()\n",
    "#np.save(save_sino_path, save_sino_array)\n",
    "#np.save(save_image_path, save_image_array)\n",
    "\n",
    "# Load the saved array and make sure it's the same size/has the same images #\n",
    "load_sino_array = np.load(save_sino_path, mmap_mode='r')\n",
    "load_image_array = np.load(save_image_path, mmap_mode='r')\n",
    "print('load_sino_array.shape: ', load_sino_array.shape)\n",
    "print('load_image_array.shape: ', load_image_array.shape)\n",
    "\n",
    "print('load_sino_array sample images')\n",
    "print('load_image_array sample images')\n",
    "show_multiple_matched_tensors(torch.from_numpy(load_sino_array[500:509]))\n",
    "show_multiple_matched_tensors(torch.from_numpy(load_image_array[500:509]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0S2M7PagXsv"
   },
   "source": [
    "# Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23TOba33L4qf"
   },
   "outputs": [],
   "source": [
    "## Find what GPU I'm using ##\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should be True\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3anZSZqZgJs"
   },
   "source": [
    "# Notes:\n",
    "Change next\n",
    "===========\n",
    "Change out HyperOpt search for Optuna\n",
    "Install Git on home machine: https://git-scm.com/install/windows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For high/low MSE experiments\n",
    "============================\n",
    "-Tuned networks for 180 minutes each.\n",
    "\n",
    "-Trained for 100 epochs using on-the-fly augmentation\n",
    "\n",
    "-See notes in checkpoint folder\n",
    "\n",
    "\n",
    "For LDM, window = 5, stride = 2\n",
    "===============================\n",
    "tune_max_t = 20            \n",
    "\n",
    "tune_minutes = 180      \n",
    "\n",
    "tune_batches_per_report=12    \n",
    "\n",
    "tune_augment=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AHpRBHQKzo9"
   },
   "source": [
    "Tensor board works for all experiments except the last one.\n",
    "My plotting function no longer works for any of the experiments."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
